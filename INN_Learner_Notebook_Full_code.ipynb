{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/farrukhtaba/farrukhtaba/blob/main/INN_Learner_Notebook_Full_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><font size=6> Bank Churn Prediction </font></center>"
      ],
      "metadata": {
        "id": "EoO7ROnuht51"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem Statement"
      ],
      "metadata": {
        "id": "Q__obHNhdHtV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Context"
      ],
      "metadata": {
        "id": "WSyQJZSAaPA3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Businesses like banks which provide service have to worry about problem of 'Customer Churn' i.e. customers leaving and joining another service provider. It is important to understand which aspects of the service influence a customer's decision in this regard. Management can concentrate efforts on improvement of service, keeping in mind these priorities."
      ],
      "metadata": {
        "id": "vJQ5k8umdJdN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Objective"
      ],
      "metadata": {
        "id": "s749lpTNaRkN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You as a Data scientist with the  bank need to  build a neural network based classifier that can determine whether a customer will leave the bank  or not in the next 6 months."
      ],
      "metadata": {
        "id": "XbrLMQQ6dKQU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Dictionary"
      ],
      "metadata": {
        "id": "Tsb28swdaVAs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* CustomerId: Unique ID which is assigned to each customer\n",
        "\n",
        "* Surname: Last name of the customer\n",
        "\n",
        "* CreditScore: It defines the credit history of the customer.\n",
        "  \n",
        "* Geography: A customer’s location\n",
        "   \n",
        "* Gender: It defines the Gender of the customer\n",
        "   \n",
        "* Age: Age of the customer\n",
        "    \n",
        "* Tenure: Number of years for which the customer has been with the bank\n",
        "\n",
        "* NumOfProducts: refers to the number of products that a customer has purchased through the bank.\n",
        "\n",
        "* Balance: Account balance\n",
        "\n",
        "* HasCrCard: It is a categorical variable which decides whether the customer has credit card or not.\n",
        "\n",
        "* EstimatedSalary: Estimated salary\n",
        "\n",
        "* isActiveMember: Is is a categorical variable which decides whether the customer is active member of the bank or not ( Active member in the sense, using bank products regularly, making transactions etc )\n",
        "\n",
        "* Exited : whether or not the customer left the bank within six month. It can take two values\n",
        "** 0=No ( Customer did not leave the bank )\n",
        "** 1=Yes ( Customer left the bank )"
      ],
      "metadata": {
        "id": "-mquomUwdMol"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True)."
      ],
      "metadata": {
        "id": "oVZRatA-W3lm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import drive to access the files on My Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "id": "oyTYkHrRc0kz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IfeZclzIHUNs"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Libraries to help with data visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "# Library to split data\n",
        "from sklearn.model_selection import train_test_split\n",
        "# library to import to standardize the data\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# To oversample and undersample data\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "#To import different metrics\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import explained_variance_score, mean_squared_error, r2_score, mean_absolute_error\n",
        "\n",
        "#Importing classback API\n",
        "from keras import callbacks\n",
        "\n",
        "# Importing tensorflow library\n",
        "import tensorflow as tf\n",
        "\n",
        "# importing different functions to build models\n",
        "from tensorflow.keras.layers import Dense, Dropout,InputLayer\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "# Importing Batch Normalization\n",
        "from keras.layers import BatchNormalization\n",
        "\n",
        "# Importing backend\n",
        "from tensorflow.keras import backend\n",
        "\n",
        "# Importing shffule\n",
        "from random import shuffle\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "# Importing optimizers\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "\n",
        "# Library to avoid the warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import and readi the data set\n",
        "data = pd.read_csv('/content/drive/My Drive/AIML Colab files/Project 7/bank.csv')"
      ],
      "metadata": {
        "id": "q0JtV1tzXAxI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display 5 sample rows\n",
        "data.sample(5)"
      ],
      "metadata": {
        "id": "7cq0z2-7YCab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create a copy of the data\n",
        "df = data.copy()"
      ],
      "metadata": {
        "id": "qKH7G4KEaQh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#drop the first 2 columns\n",
        "df.drop(['RowNumber','CustomerId','Surname'], axis=1, inplace=True)\n",
        "#display the data frane columns to confirm the drop command wss successful\n",
        "df.columns"
      ],
      "metadata": {
        "id": "hL3qW2jcaTS4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is observed that the first two columns 'RowNumber' and 'CustomerId' are unique for evry customer, hence they do not count as good predictors for our classification problem. The Surname is a variable that is common between customers just by chance and is not considered a valuable predictor as well. Therefore, those three columns need to be dropped."
      ],
      "metadata": {
        "id": "s9vpmNSrYWzO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#check if the dataset has null values\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "9MGOfO7JaVRk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check if the dataset has duplicated values\n",
        "df.duplicated().sum()"
      ],
      "metadata": {
        "id": "ZAzqk0k0nMcT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#display the data types of all variables\n",
        "df.dtypes"
      ],
      "metadata": {
        "id": "71l2dfxXnVz8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Derive the 5 point summary of the data\n",
        "df.describe(include='all')"
      ],
      "metadata": {
        "id": "VItUFwconbnw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are two categorical variables:\n",
        "\n",
        "<ul><li>Geography: Contains 3 unique values, with \"France\" being the most common, representing 50% of the customers.</ul>\n",
        "<ul><li>Gender: Includes 2 unique values, with \"Male\" being the most frequent.\n",
        "Additional categorical indicators:</ul>\n",
        "\n",
        "<ul><li>HasCrCard: 50% of customers hold a credit card.</ul>\n",
        "<ul><li>IsActiveMember: 50% of customers are active members.</ul>\n",
        "<ul><li>Exited: This is the target variable, with at least 75% of customers remaining with the bank.</ul>\n",
        "\n",
        "<ul>The remaining five variables are numerical:</ul>\n",
        "\n",
        "<ul><li>CreditScore: Normally distributed, ranging from 350 to 850, with an average of 650.</ul>\n",
        "<ul><li>Age: Right-skewed, with values from 18 to 850 and a mean of 92.</ul>\n",
        "<ul><li>Tenure: Normally distributed, with values from 0 to 850 and an average of 10.</ul>\n",
        "<ul><li>Balance: Left-skewed, ranging from 0 to 250,898, and an average of 97,198.</ul>\n",
        "<ul><li>EstimatedSalary: Left-skewed, with values from 11.58 to 199,992 and a mean of 100,193.</ul>\n",
        "<ul>These observations will be validated through Exploratory Data Analysis (EDA).</ul>\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kKzsgbJxoo65"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b>Exploratory Data Analysis</b>\n",
        "\n",
        "1. Univariate Analysis</br>\n",
        "\n",
        "EDA For the numerical variables\n",
        "We will create a function to plot the histogram and box plot for all variables (note that all variables are numerical)"
      ],
      "metadata": {
        "id": "msG02RmXq3tK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def histogram_boxplot(data, feature, figsize=(12, 7), kde=False, bins=None):\n",
        "    \"\"\"\n",
        "    Boxplot and histogram combined\n",
        "\n",
        "    data: dataframe\n",
        "    feature: dataframe column\n",
        "    figsize: size of figure (default (12,7))\n",
        "    kde: whether to the show density curve (default False)\n",
        "    bins: number of bins for histogram (default None)\n",
        "    \"\"\"\n",
        "    f2, (ax_box2, ax_hist2) = plt.subplots(\n",
        "        nrows=2,  # Number of rows of the subplot grid= 2\n",
        "        sharex=True,  # x-axis will be shared among all subplots\n",
        "        gridspec_kw={\"height_ratios\": (0.25, 0.75)},\n",
        "        figsize=figsize,\n",
        "    )  # creating the 2 subplots\n",
        "    sns.boxplot(\n",
        "        data=data, x=feature, ax=ax_box2, showmeans=True, color=\"violet\"\n",
        "    )  # boxplot will be created and a star will indicate the mean value of the column\n",
        "    sns.histplot(\n",
        "        data=data, x=feature, kde=kde, ax=ax_hist2, bins=bins, palette=\"winter\"\n",
        "    ) if bins else sns.histplot(\n",
        "        data=data, x=feature, kde=kde, ax=ax_hist2, discrete=True\n",
        "    )  # For histogram\n",
        "    ax_hist2.axvline(\n",
        "        data[feature].mean(), color=\"green\", linestyle=\"--\"\n",
        "    )  # Add mean to the histogram\n",
        "    ax_hist2.axvline(\n",
        "        data[feature].median(), color=\"black\", linestyle=\"-\"\n",
        "    )  # Add median to the histogram"
      ],
      "metadata": {
        "id": "xbPAxzCkra-Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a list of numerical columns\n",
        "numerical = ['CreditScore', 'Age', 'Balance','EstimatedSalary']\n",
        "\n",
        "# ploting the numerical variables\n",
        "for i in numerical:\n",
        "    histogram_boxplot(df, i, kde=True, figsize=(8, 4))"
      ],
      "metadata": {
        "id": "cMb2FSmirhns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b>Observations</b><br>\n",
        "\n",
        "<ul><li>Credit Score: The box plot indicates that the mean and median are aligned, though some outliers appear beyond the left whisker. This raises the question of whether these outliers represent customers who leave the bank, which will be explored in the bivariate analysis. The outliers are consistent with the dataset and don’t require removal.</ul>\n",
        "\n",
        "<ul><li>Age: The distribution is right-skewed, with several outliers beyond the right whisker. These outliers are also consistent with the data and do not need to be removed.</ul>\n",
        "\n",
        "<ul><li>Balance: The box plot shows a right-skewed distribution with no outliers. However, the histogram lacks visual clarity, so it will be re-plotted separately for more detailed analysis.</ul>\n",
        "\n",
        "<ul><li>Estimated Salary: The box plot suggests an almost perfect normal distribution, with mean and median alignment and whiskers of roughly equal length. Since the histogram is not visually clear, it will be re-plotted on a logarithmic scale for better insight.</ul>"
      ],
      "metadata": {
        "id": "m3H3BOxzrqC5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#plotting the data on a logarithmic scale\n",
        "plt.hist(df['Balance']);\n",
        "plt.title('Balance');"
      ],
      "metadata": {
        "id": "nT0R30qisVGX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The balance histogram is right skewed and shows a significant number of customers (approx. 3500) with 0 balance. The remaining customers show adopt an almost normally distributed balance values. This indicates there are two different customer clusters at least. It is interesting to study further how these cluster vary with respect to our target variable in the bivariate analysis.\n",
        "\n",
        "Estinated Salary distribution on a logarithmic scale:"
      ],
      "metadata": {
        "id": "YHvYFvthsbbT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#plotting the data on a logarithmic scale\n",
        "plt.hist(np.log(df['EstimatedSalary']));\n",
        "plt.title('log(EstimatedSalary)');"
      ],
      "metadata": {
        "id": "kBqWh-VusgPY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.boxplot(np.log(df['EstimatedSalary']));"
      ],
      "metadata": {
        "id": "iyuN2qoosm1g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "On a logarithmic scale, the Estimated Salary is heavily left skewed with a high number of outliers beyond the left skewer. The outliers still seem consistent with the reality situation as a number of customers can be of high age and retired with no income salary or non-working customers."
      ],
      "metadata": {
        "id": "PGX10jQjsvwM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b>EDA For the categorical variables</b><br>\n",
        "We will create a function to plot labelled barplots for all variables (note that all variables are categorical)"
      ],
      "metadata": {
        "id": "9GuxZhetsxEF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# funtcion to plot labelled barplot\n",
        "def labeled_barplot(data, feature, perc=False, n=None):\n",
        "    \"\"\"\n",
        "    Barplot with percentage at the top\n",
        "\n",
        "    data: dataframe\n",
        "    feature: dataframe column\n",
        "    perc: whether to display percentages instead of count (default is False)\n",
        "    n: displays the top n category levels (default is None, i.e., display all levels)\n",
        "    \"\"\"\n",
        "\n",
        "    total = len(data[feature])  # length of the column\n",
        "    count = data[feature].nunique()\n",
        "    if n is None:\n",
        "        plt.figure(figsize=(count + 1, 5))\n",
        "    else:\n",
        "        plt.figure(figsize=(n + 1, 5))\n",
        "\n",
        "    plt.xticks(rotation=90, fontsize=15)\n",
        "    ax = sns.countplot(\n",
        "        data=data,\n",
        "        x=feature,\n",
        "        palette=\"Paired\",\n",
        "        order=data[feature].value_counts().index[:n].sort_values(),\n",
        "    )\n",
        "\n",
        "    for p in ax.patches:\n",
        "        if perc == True:\n",
        "            label = \"{:.1f}%\".format(\n",
        "                100 * p.get_height() / total\n",
        "            )  # percentage of each class of the category\n",
        "        else:\n",
        "            label = p.get_height()  # count of each level of the category\n",
        "\n",
        "        x = p.get_x() + p.get_width() / 2  # width of the plot\n",
        "        y = p.get_height()  # height of the plot\n",
        "\n",
        "        ax.annotate(\n",
        "            label,\n",
        "            (x, y),\n",
        "            ha=\"center\",\n",
        "            va=\"center\",\n",
        "            size=12,\n",
        "            xytext=(0, 5),\n",
        "            textcoords=\"offset points\",\n",
        "        )  # annotate the percentage\n",
        "\n",
        "    plt.show()  # show the plot"
      ],
      "metadata": {
        "id": "QXZECyc9tEGk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a list of categorical columns\n",
        "categorical = ['Geography', 'Gender', 'NumOfProducts', 'HasCrCard', 'IsActiveMember','Exited']\n",
        "# ploting the categorical variables\n",
        "for i in categorical:\n",
        "    labeled_barplot(df, i, perc=True)"
      ],
      "metadata": {
        "id": "TUypD0eEtFiZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b>Observations</b><br>\n",
        "<ul><li>Geography: 50% of customers are from France, 25% from Germany, and 25% from Spain.</ul></li>\n",
        "\n",
        "<ul><li>Gender: Approximately 55% of customers are male, with the remainder female.</ul></li>\n",
        "\n",
        "<ul><li>NumOfProducts: Half of the customers use only one product, around 46% use two products, and the remaining 3.3% use three to four products.</ul></li>\n",
        "\n",
        "<ul><li>HasCrCard: Most customers (70%) have credit cards.</ul></li>\n",
        "\n",
        "<ul><li>IsActiveMember: Nearly half of the customers are active members, with a slight majority being active.</ul></li>\n",
        "\n",
        "<ul><li>Exited: Around 20% of customers have exited, while 80% have not. This indicates an imbalance in the target</ul></li>"
      ],
      "metadata": {
        "id": "IsqM5N_qtmhM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b>2. Bivariate analysis</b><br><br>\n",
        "<ul><li>Use appropriate visualizations to identify the patterns and insights - Any other exploratory deep dive<br></ul></li>\n",
        "<ul><li>Start by plotting the pair plot and heatmap to observe if there are any correlations between variables and/or clear clusterring.</ul></li>"
      ],
      "metadata": {
        "id": "gfpWLmOuuHvS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15,15))\n",
        "sns.pairplot(df, hue=\"Exited\",diag_kind='kde')"
      ],
      "metadata": {
        "id": "npx7bnHxuWcm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "sns.heatmap(df.corr(),vmax=1,vmin=-1,annot=True)"
      ],
      "metadata": {
        "id": "fKYVAnZSu2HK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b>Observations:</b><br>\n",
        "<ul><li>The pair plot and heat map reveal no notable correlations between the variables.</ul></li>\n",
        "<ul><li>The pair plot indicates distinct clustering patterns in certain variables: Balance (2 clusters), Num of Products (3-4 clusters), HasCrCards (2 clusters), and IsActiveMember (2 clusters).</ul></li>\n",
        "<ul><li>Both exited and non-exited customers are represented across all clusters. However, due to class imbalance, exited customers are significantly fewer than non-exited customers.</ul></li>"
      ],
      "metadata": {
        "id": "TZ6qqVBZvNC-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### function to plot distributions wrt target\n",
        "\n",
        "\n",
        "def distribution_plot_wrt_target(data, predictor, target):\n",
        "\n",
        "    fig, axs = plt.subplots(2, 2, figsize=(12, 10))\n",
        "\n",
        "    target_uniq = data[target].unique()\n",
        "\n",
        "    axs[0, 0].set_title(\"Distribution of target for target=\" + str(target_uniq[0]))\n",
        "    sns.histplot(\n",
        "        data=data[data[target] == target_uniq[0]],\n",
        "        x=predictor,\n",
        "        kde=False,\n",
        "        ax=axs[0, 0],\n",
        "        color=\"teal\",\n",
        "        stat=\"density\",\n",
        "    )\n",
        "\n",
        "    axs[0, 1].set_title(\"Distribution of variable for target=\" + str(target_uniq[1]))\n",
        "    sns.histplot(\n",
        "        data=data[data[target] == target_uniq[1]],\n",
        "        x=predictor,\n",
        "        kde=False,\n",
        "        ax=axs[0, 1],\n",
        "        color=\"orange\",\n",
        "        stat=\"density\",\n",
        "    )\n",
        "\n",
        "    axs[1, 0].set_title(\"Boxplot w.r.t target\")\n",
        "    sns.boxplot(data=data, x=target, y=predictor, ax=axs[1, 0], palette=\"gist_rainbow\")\n",
        "\n",
        "    axs[1, 1].set_title(\"Boxplot (without outliers) w.r.t target\")\n",
        "    sns.boxplot(\n",
        "        data=data,\n",
        "        x=target,\n",
        "        y=predictor,\n",
        "        ax=axs[1, 1],\n",
        "\n",
        "        showfliers=False,\n",
        "        palette=\"gist_rainbow\",\n",
        "    )\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "axv1_awlve4X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b>Observations:</b><br>\n",
        "<ul><li>CreditScore: The distribution of this variable is similar across both classes of the target variable, and the presence of outliers does not impact the distribution. Therefore, CreditScore is a poor predictor.</ul></li>\n",
        "<ul><li>Age: The median age is higher for customers who exited the bank compared to those who stayed, with more outliers among the exited customers. This suggests that customers around the median age of 45 are more likely to leave the bank. Age is thus considered a weak predictor of the target variable.</ul></li>\n",
        "<ul><li>Balance: There is a high concentration of customers with a balance of zero among those who exited. Additionally, the interquartile range (IQR) for balance is narrower for exited customers compared to those who stayed, and the median balance is slightly higher for those who exited. Consequently, Balance is regarded as a very weak predictor.</ul></li>\n",
        "<ul><li>EstimatedSalary: This variable has a nearly uniform distribution across both classes, indicating little predictive power.</ul></li>"
      ],
      "metadata": {
        "id": "www6GMdmwnDl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in numerical:\n",
        "  distribution_plot_wrt_target(df, i, 'Exited')"
      ],
      "metadata": {
        "id": "b-qiUEOcuDGu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to plot stacked bar chart\n",
        "\n",
        "\n",
        "def stacked_barplot(data, predictor, target):\n",
        "    \"\"\"\n",
        "    Print the category counts and plot a stacked bar chart\n",
        "\n",
        "    data: dataframe\n",
        "    predictor: independent variable\n",
        "    target: target variable\n",
        "    \"\"\"\n",
        "    count = data[predictor].nunique()\n",
        "    sorter = data[target].value_counts().index[-1]\n",
        "    tab1 = pd.crosstab(data[predictor], data[target], margins=True).sort_values(\n",
        "        by=sorter, ascending=False\n",
        "    )\n",
        "    print(tab1)\n",
        "    print(\"-\" * 120)\n",
        "    tab = pd.crosstab(data[predictor], data[target], normalize=\"index\").sort_values(\n",
        "        by=sorter, ascending=False\n",
        "    )\n",
        "    tab.plot(kind=\"bar\", stacked=True, figsize=(count + 5, 6))\n",
        "    plt.legend(\n",
        "        loc=\"lower left\", frameon=False,\n",
        "    )\n",
        "    plt.legend(loc=\"upper left\", bbox_to_anchor=(1, 1))\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "N-968ZcIxIHG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b>Observations<b><br>\n",
        "<ul><li>Geography: The highest proportion of exited customers comes from Germany, while Spain and France show almost identical exit ratios. This variable is considered a reasonably good predictor.</ul></li>\n",
        "<ul><li>Gender: A greater proportion of exited customers are female, making Gender a fairly good predictor.</ul></li>\n",
        "<ul><li>NumOfProducts: All customers with 4 products, as well as a large proportion of those with 3 products, have exited. The lowest exit ratio is among customers with 2 products, indicating strong predictive power for this variable.</ul></li>\n",
        "<ul><li>HasCrCard: The exit ratio is nearly the same for customers with and without credit cards, making this a very poor predictor.</ul></li>\n",
        "<ul><li>IsActiveMember: A higher proportion of non-active customers have exited compared to active ones, making this a fairly good predictor.</ul></li>"
      ],
      "metadata": {
        "id": "c6mZ3wOVxhLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "categorical = ['Geography', 'Gender', 'NumOfProducts', 'HasCrCard', 'IsActiveMember']\n",
        "\n",
        "# ploting the categorical variables\n",
        "for i in categorical:\n",
        "    stacked_barplot(df, i, 'Exited')"
      ],
      "metadata": {
        "id": "ZrKYY04Zx3ZM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b>EDA Insights</b><br><br>\n",
        "From the Univariate Analysis we conclude:\n",
        "\n",
        "<ul><li>All bank customers come are based in France, Germany or Spain and their Age vary from 18 to 92 years old.</ul></li>\n",
        "<ul><li>The majority of customers are Males, from France, have credit cards and have not exited the bank</ul></li>\n",
        "From the Bivariate Anslysis we conclude:\n",
        "\n",
        "<ul><li>There is no colinearity or correlation observed between features.</ul></li>\n",
        "<ul><li>The fairly good predictors are: Age, Geography, Gender, Num of products and IsActiveMember</ul></li>\n",
        "<ul><li>Feature of customers who are most likely to exit the bank are :<br><br>\n",
        "<ul><li>Age: Ranges from 28 to 52, with a median of 45</ul></li>\n",
        "<ul><li>Location: Germany</ul></li>\n",
        "<ul><li>Gender: Female</ul></li>\n",
        "<ul><li>Number of Products: 3 to 4</ul></li>\n",
        "<ul><li>Active Member: No</ul></li>"
      ],
      "metadata": {
        "id": "kHb358LKyA5w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b>Data Pre-processing</b><br><br>\n",
        "Splitting the target variable and predictors"
      ],
      "metadata": {
        "id": "7q4kySFp3sr-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = df.drop('Exited',axis=1)\n",
        "y = df['Exited']"
      ],
      "metadata": {
        "id": "LWosPVLM28Kz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Applying one-hot-encoding for the categorical features"
      ],
      "metadata": {
        "id": "Ym0lTVu233P8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dummy_cat = ['Geography','Gender']\n",
        "X = pd.get_dummies(X,columns=dummy_cat,drop_first= True)\n",
        "X.head()"
      ],
      "metadata": {
        "id": "jS4w5OGb37z_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.Scaling the numerical data using zscore to unite the distribution of all variables to a mean of 0 and a std of 1.\n"
      ],
      "metadata": {
        "id": "R9gqbNLM3_sd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# to scale the data\n",
        "from scipy.stats import zscore\n",
        "X_Scaled = X.apply(zscore)\n",
        "X_Scaled.head()"
      ],
      "metadata": {
        "id": "42nfdLdd4CZY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split the data to test and train"
      ],
      "metadata": {
        "id": "hyEhZ58k4H_q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the non-scaled data into train and test\n",
        "X_train_ns, X_test_ns, y_train_ns, y_test_ns = train_test_split(X, y, test_size=0.2, random_state=1, stratify=y)"
      ],
      "metadata": {
        "id": "Tn4g2NEe4KUs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the scaled data into train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_Scaled, y, test_size=0.2, random_state=1, stratify=y)"
      ],
      "metadata": {
        "id": "NTH5eCQk4Quc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#displaying the shape of train and test data sets to confirm the split was successful\n",
        "print(f''' X_train shape: {X_train.shape}\n",
        " X_test shape: {X_test.shape}\n",
        " y_train shape: {y_train.shape}\n",
        " y_test shape: {y_test.shape}''')"
      ],
      "metadata": {
        "id": "Gwhy1Goc4be6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.Data Oversampling: To balance the target variable class ration we will prepare an oversampled train and test datasets using the SMOTE algorithm train data is zscore scaled"
      ],
      "metadata": {
        "id": "xlzbypKh4h7s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Before UpSampling, counts of label 'Yes': {}\".format(sum(y_train == 1)))\n",
        "print(\"Before UpSampling, counts of label 'No': {} \\n\".format(sum(y_train == 0)))\n",
        "\n",
        "sm = SMOTE(sampling_strategy=1, k_neighbors=5, random_state=1)\n",
        "\n",
        "X_train_over, y_train_over = sm.fit_resample(X_train, y_train)\n",
        "\n",
        "print(\"After UpSampling, counts of label 'Yes': {}\".format(sum(y_train_over == 1)))\n",
        "print(\"After UpSampling, counts of label 'No': {} \\n\".format(sum(y_train_over == 0)))\n",
        "\n",
        "\n",
        "print(\"After UpSampling, the shape of X_train: {}\".format(X_train_over.shape))\n",
        "print(\"After UpSampling, the shape of y-train: {} \\n\".format(y_train_over.shape))"
      ],
      "metadata": {
        "id": "L-zkfE4m4nUH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.Data Undersampling: To balance the target variable class ration we will prepare an undersampled train and test datasets using the Random Under Sampler noting that the train data is zscore scaled"
      ],
      "metadata": {
        "id": "BDMpDlqM4sNd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "applting the random under sampler using sampling strategy 0.5\n",
        "random_us = RandomUnderSampler(random_state=1, sampling_strategy = 0.5)\n",
        "X_train_un, y_train_un = random_us.fit_resample(X_train, y_train)\n",
        "\n",
        "print(\"After UpSampling, counts of label 'Yes': {}\".format(sum(y_train_un == 1)))\n",
        "print(\"After UpSampling, counts of label 'No': {} \\n\".format(sum(y_train_un == 0)))\n",
        "\n",
        "\n",
        "print(\"After UpSampling, the shape of X_train: {}\".format(X_train_un.shape))\n",
        "print(\"After UpSampling, the shape of y-train: {} \\n\".format(y_train_un.shape))"
      ],
      "metadata": {
        "id": "AgDBxqxK4udv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset now is ready for modeling noting that the over sampled and under sampled data will be utilized only if required to enhance the model performance."
      ],
      "metadata": {
        "id": "TNzaR0jp44Fu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b>Model building</b><br><br>\n",
        "Our key performance metric is the Recall as we are aiming to reduce the False Negatives and hence, predict correctly the customers that are most likely to exit the bank and take early actions to avoid this from occuring. We will start by a very simple model using the rule of thumb parameters as initial parameters, observe the performance and start enhancing it."
      ],
      "metadata": {
        "id": "hnNI9oaQ5cy5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "backend.clear_session()\n",
        "#Fixing the seed for random number generators so that we can ensure we receive the same output everytime\n",
        "np.random.seed(1)\n",
        "import random\n",
        "random.seed(1)\n",
        "tf.random.set_seed(1)"
      ],
      "metadata": {
        "id": "nnFN-D2N5lMt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b>Model_01:</b><br>\n",
        "<ul><li>Hidden layers = 2</ul></li>\n",
        "<ul><li>Activation function for hidden layers = Relu</ul></li>\n",
        "<ul><li>Output layer with 1 nodes</ul></li>\n",
        "<ul><li>Output Activation function = Sigmoid</ul></li>\n",
        "<ul><li>Output loss function = Cross Entropy (since this is a classification problem)</ul></li>\n",
        "<ul><li>Optimizer = Adam</ul></li>\n",
        "<ul><li>epochs = 20</ul></li>\n",
        "<ul><li>Data is scaled using zscore</ul></li>"
      ],
      "metadata": {
        "id": "yParpgDQ5qzs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model summary\n",
        "model_01.summary()"
      ],
      "metadata": {
        "id": "W2SHYt0_6Bcc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compiling the model with 'cross entropy' as loss function, 'Adam' Optimizer and 'accuracy' metrics\n",
        "model_01.compile(loss='binary_crossentropy',\n",
        "              optimizer='Adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Fitting the model on train and validation with 20 epochs\n",
        "fitted_model_01 = model_01.fit(X_train, y_train, validation_split=0.2,epochs=20)"
      ],
      "metadata": {
        "id": "xP-HoEUq6KMx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plotting Train Loss vs Validation Loss\n",
        "plt.plot(fitted_model_01.history['loss'])\n",
        "plt.plot(fitted_model_01.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5SBOltv26QOt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b>Observation:</b><br> <br>The error difference between the validation and train set is increasing, hence this indicates overfitting which requires handling. We shall observe further the model performance on other metrics in order to decinde on the next step.<br>\n",
        "\n",
        "Deriving more metrics"
      ],
      "metadata": {
        "id": "uABsjJdq6WWi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# defining a function to display the confusion matrix\n",
        "def make_confusion_matrix(cf,\n",
        "                          group_names=None,\n",
        "                          categories='auto',\n",
        "                          count=True,\n",
        "                          percent=True,\n",
        "                          cbar=True,\n",
        "                          xyticks=True,\n",
        "                          xyplotlabels=True,\n",
        "                          sum_stats=True,\n",
        "                          figsize=None,\n",
        "                          cmap='Blues',\n",
        "                          title=None):\n",
        "    '''\n",
        "    This function will make a pretty plot of an sklearn Confusion Matrix cm using a Seaborn heatmap visualization.\n",
        "    Arguments\n",
        "    '''\n",
        "\n",
        "\n",
        "    # CODE TO GENERATE TEXT INSIDE EACH SQUARE\n",
        "    blanks = ['' for i in range(cf.size)]\n",
        "\n",
        "    if group_names and len(group_names)==cf.size:\n",
        "        group_labels = [\"{}\\n\".format(value) for value in group_names]\n",
        "    else:\n",
        "        group_labels = blanks\n",
        "\n",
        "    if count:\n",
        "        group_counts = [\"{0:0.0f}\\n\".format(value) for value in cf.flatten()]\n",
        "    else:\n",
        "        group_counts = blanks\n",
        "\n",
        "    if percent:\n",
        "        group_percentages = [\"{0:.2%}\".format(value) for value in cf.flatten()/np.sum(cf)]\n",
        "    else:\n",
        "        group_percentages = blanks\n",
        "\n",
        "    box_labels = [f\"{v1}{v2}{v3}\".strip() for v1, v2, v3 in zip(group_labels,group_counts,group_percentages)]\n",
        "    box_labels = np.asarray(box_labels).reshape(cf.shape[0],cf.shape[1])\n",
        "\n",
        "\n",
        "    # CODE TO GENERATE SUMMARY STATISTICS & TEXT FOR SUMMARY STATS\n",
        "    if sum_stats:\n",
        "        #Accuracy is sum of diagonal divided by total observations\n",
        "        accuracy  = np.trace(cf) / float(np.sum(cf))\n",
        "\n",
        "\n",
        "\n",
        "    # SET FIGURE PARAMETERS ACCORDING TO OTHER ARGUMENTS\n",
        "    if figsize==None:\n",
        "        #Get default figure size if not set\n",
        "        figsize = plt.rcParams.get('figure.figsize')\n",
        "\n",
        "    if xyticks==False:\n",
        "        #Do not show categories if xyticks is False\n",
        "        categories=False\n",
        "\n",
        "\n",
        "    # MAKE THE HEATMAP VISUALIZATION\n",
        "    plt.figure(figsize=figsize)\n",
        "    sns.heatmap(cf,annot=box_labels,fmt=\"\",cmap=cmap,cbar=cbar,xticklabels=categories,yticklabels=categories)\n",
        "\n",
        "\n",
        "    if title:\n",
        "        plt.title(title)"
      ],
      "metadata": {
        "id": "Eeez4rod60YN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtain the y_predict from the X_test\n",
        "y_pred=model_01.predict(X_test)\n",
        "\n",
        "#set the threshold to 0.5\n",
        "y_pred = (y_pred > 0.5)\n",
        "\n",
        "# Display the classification report\n",
        "from sklearn import metrics\n",
        "cr=metrics.classification_report(y_test,y_pred)\n",
        "print(cr)"
      ],
      "metadata": {
        "id": "opL8f-Vl62NA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Macro Averge of the Recall is considerably low and since this is still our first model, we can sure enhance this performance. Let us observe the confusion matrix."
      ],
      "metadata": {
        "id": "rdX6kW1069Y-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#displaying the confusion matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm=confusion_matrix(y_test, y_pred)\n",
        "labels = ['True Positive','False Negative','False Positive','True Negative']\n",
        "categories = [ 'Not Exiting','Exiting']\n",
        "make_confusion_matrix(cm,\n",
        "                      group_names=labels,\n",
        "                      categories=categories,\n",
        "                      cmap='Blues')"
      ],
      "metadata": {
        "id": "uBKIbiBX7E5i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The FN percentage (4.55%) is considerably high when compared to the TN percentage (10.5%) and this is expected since the target classes are imbalanced. Yet, 4.5% of FN is low compared to the whole data set. Our next step is to use the oversampled train data set to train the same Neural Network created in Model_01 and observe if there is a performance improvement."
      ],
      "metadata": {
        "id": "Mj7_9wGU7Lym"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b>Model Performance Improvement</b><br><br>\n",
        "Moving on forward, a multiple models will be created using different combinations of hyperparameters including:\n",
        "\n",
        "<ul><li>Type of Optimizer</ul></li>\n",
        "<ul><li>Number of Layers</ul></li>\n",
        "<ul><li>Number of Neurons in a layer</ul></li>\n",
        "<ul><li>Weight initialization</ul></li>\n",
        "<ul><li>Regularization techniques (Dropout and Batch Normalization)</ul></li>\n",
        "<ul><li>Learning Rate</ul></li>\n",
        "<ul><li>Batch Size</ul></li><br>\n",
        "Also, we will utilize the GridSeachCV and Keras Tuner to tune some of the above hyperparameters.<br>\n",
        "\n",
        "Finally, we will keep in mind that the data target classes are imbalanced, hence we will utilize the over_sampled and under_sampled data prepared in the data pre-processing step whenever required.<br>\n",
        "\n",
        "In each new model, the hyperparameter will be stated, the model performance will be evaluated on several metrics including the our most important metric which is the Recall."
      ],
      "metadata": {
        "id": "i1rQXvyM7NBy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b>Model_2:<br><br>\n",
        "<ul><li>Hidden layers = 2</ul></li>\n",
        "<ul><li>Activation function for hidden layers = Relu</ul></li>\n",
        "<ul><li>Output layer with 1 nodes</ul></li>\n",
        "<ul><li>Outout Activation function = Sigmoid</ul></li>\n",
        "<ul><li>Output loss function = Cross Entropy (since this is a classification problem)</ul></li>\n",
        "<ul><li>Optimizer = Adam</ul></li>\n",
        "<ul><li>epochs = 20</ul></li>\n",
        "<ul><li>Data is scaled using zscore</ul></li>\n",
        "<ul><li>Training the model on the oversampled train data set (using SMOTE)</ul></li>"
      ],
      "metadata": {
        "id": "VJKl9WMu72re"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "backend.clear_session()\n",
        "#Fixing the seed for random number generators so that we can ensure we receive the same output everytime\n",
        "np.random.seed(1)\n",
        "random.seed(1)\n",
        "tf.random.set_seed(1)"
      ],
      "metadata": {
        "id": "cO6cyeFv8Wk9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing the model\n",
        "model_02 = Sequential()\n",
        "# Adding the first hidden layer with 64 neurons, relu as activation function\n",
        "model_02.add(Dense(64, activation='relu', input_dim=11))\n",
        "# Adding the second hidden layer with 32 neurons, relu as activation function\n",
        "model_02.add(Dense(32, activation='relu'))\n",
        "# Adding the output layer with one neuron and Sigmoid as activation\n",
        "model_02.add(Dense(1, activation='sigmoid'))"
      ],
      "metadata": {
        "id": "MCgDah_S8af3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compiling the model with 'cross entropy' as loss function, 'Adam' Optimizer and 'accuracy' metrics\n",
        "model_02.compile(loss='binary_crossentropy',\n",
        "              optimizer='Adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Fitting the model on train and validation with 20 epochs\n",
        "fitted_model_02 = model_02.fit(X_train_over, y_train_over, validation_split=0.2,epochs=20)\n",
        "\n",
        "#Plotting Train Loss vs Validation Loss\n",
        "plt.plot(fitted_model_02.history['loss'])\n",
        "plt.plot(fitted_model_02.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "358FiEHk8eNu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are very significant noise on the validation set and the error between validation and training is still sigificantly high. Hence, it appears tha over sampling has not trained the model in a better way. Let us calculate the metrics and confusion matrix."
      ],
      "metadata": {
        "id": "uGv1lqso8kw-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtain the y_predict from the X_test\n",
        "y_pred=model_02.predict(X_test)\n",
        "\n",
        "#set the threshold to 0.5\n",
        "y_pred = (y_pred > 0.5)\n",
        "\n",
        "# Display the classification report\n",
        "cr=metrics.classification_report(y_test,y_pred)\n",
        "print(cr)\n",
        "\n",
        "#Calculating the confusion matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm=confusion_matrix(y_test, y_pred)\n",
        "labels = ['True Positive','False Negative','False Positive','True Negative']\n",
        "categories = [ 'Not Exiting','Exiting']\n",
        "make_confusion_matrix(cm,\n",
        "                      group_names=labels,\n",
        "                      categories=categories,\n",
        "                      cmap='Blues')"
      ],
      "metadata": {
        "id": "Gli4exwb8sxo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b>Observation:</b><br><br>\n",
        "\n",
        "<ul><li>Training the model using over sampled data has not improved the model performance significantly and added noise on the validation set. Although the Recall has improved from 0.73 to 0.75, the FN percentage has increased and the overfit has not improved.<br><br>\n",
        "<b>Conclusion:</b><br>\n",
        "\n",
        "<ul><li>The NN requires hyperparameter tuning which will be out next step.</ul></li>\n",
        "<b>Notes:</b><br>\n",
        "\n",
        "<ul><li>A trial was conducted using the undersampled training data on model_02 parameters, yet it resulted in lower Recall of 0.70, hence it was not included in this notebook.</ul></li>\n",
        "<ul><li>A trial was conducted using 50 ephochs on the over sampled training data and same model_02 parameters, yet it resulted in lower Recall of 0.73, hence it was not included in this notebook.</ul></li>"
      ],
      "metadata": {
        "id": "2wDieTqy8yzr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b>Model_03:</b><br><br>\n",
        "In this model we will use the basic training data (not oversampled) and change only the optimize to SGD instead of Adam\n",
        "\n",
        "<ul><li>Hidden layers = 2</ul></li>\n",
        "<ul><li>Activation function for hidden layers = Relu</ul></li>\n",
        "<ul><li>Output layer with 1 nodes</ul></li>\n",
        "<ul><li>Outout Activation function = Sigmoid</ul></li>\n",
        "<ul><li>Output loss function = Cross Entropy (since this is a classification problem)</ul></li>\n",
        "<ul><li>Optimizer = SGD</ul></li>\n",
        "<ul><li>epochs = 20</ul></li>\n",
        "<ul><li>Data is scaled using zscore</ul></li>"
      ],
      "metadata": {
        "id": "hFCg7gxj91Xd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "backend.clear_session()\n",
        "#Fixing the seed for random number generators so that we can ensure we receive the same output everytime\n",
        "np.random.seed(1)\n",
        "random.seed(1)\n",
        "tf.random.set_seed(1)"
      ],
      "metadata": {
        "id": "0raxbCpI-cZC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing the model\n",
        "model_03 = Sequential()\n",
        "# Adding the first hidden layer with 64 neurons, relu as activation function\n",
        "model_03.add(Dense(64, activation='relu', input_dim=11))\n",
        "# Adding the second hidden layer with 32 neurons, relu as activation function\n",
        "model_03.add(Dense(32, activation='relu'))\n",
        "# Adding the output layer with one neuron and Sigmoid as activation\n",
        "model_03.add(Dense(1, activation='sigmoid'))"
      ],
      "metadata": {
        "id": "QgwLWpZc-kKm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compiling the model with 'cross entropy' as loss function, 'Adam' Optimizer and 'accuracy' metrics\n",
        "model_03.compile(loss='binary_crossentropy',\n",
        "              optimizer='SGD',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "# Fitting the model on train and validation with 20 epochs\n",
        "fitted_model_03 = model_03.fit(X_train, y_train, validation_split=0.2,epochs=20)"
      ],
      "metadata": {
        "id": "K1rj5pUA-neQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plotting Train Loss vs Validation Loss\n",
        "plt.plot(fitted_model_03.history['loss'])\n",
        "plt.plot(fitted_model_03.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jerV-Iof-3kv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The error loss in validation and train sets is very close and much smoother (less noise). Let us explore how the performance metrics vary\n",
        "\n"
      ],
      "metadata": {
        "id": "WbFkqqWW_A_r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtain the y_predict from the X_test\n",
        "y_pred=model_03.predict(X_test)\n",
        "\n",
        "#set the threshold to 0.5\n",
        "y_pred = (y_pred > 0.5)\n",
        "\n",
        "# Display the classification report\n",
        "cr=metrics.classification_report(y_test,y_pred)\n",
        "print(cr)"
      ],
      "metadata": {
        "id": "5jpF3Yx__CFC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The recall has actually decreased when compared to the previous models. Let us display the confusion matrix and observe how the FN count changed.\n",
        "\n"
      ],
      "metadata": {
        "id": "e9WwSYmG_mYe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculating the confusion matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm=confusion_matrix(y_test, y_pred)\n",
        "labels = ['True Positive','False Negative','False Positive','True Negative']\n",
        "categories = [ 'Not Exiting','Exiting']\n",
        "make_confusion_matrix(cm,\n",
        "                      group_names=labels,\n",
        "                      categories=categories,\n",
        "                      cmap='Blues')"
      ],
      "metadata": {
        "id": "JJ9Go3b7_nMR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The FN count has decreased yet the target variable imbalance is still affecting the figures. Let us train the same model on the oversampled data and observe the difference.\n",
        "\n"
      ],
      "metadata": {
        "id": "6Jnfv2kv_57w"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "H9B0nE2J-cBX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b>Model_04:</b><br><br>\n",
        "In this model we will use the oversampled train data and keep the optimizer as SGD instead of Adam\n",
        "\n",
        "<ul><li>Hidden layers = 2</ul></li>\n",
        "<ul><li>Activation function for hidden layers = Relu</ul></li>\n",
        "<ul><li>Output layer with 1 nodes</ul></li>\n",
        "<ul><li>Outout Activation function = Sigmoid</ul></li>\n",
        "<ul><li>Output loss function = Cross Entropy (since this is a classification problem)</ul></li>\n",
        "<ul><li>Optimizer = SGD</ul></li>\n",
        "<ul><li>epochs = 20</ul></li>\n",
        "<ul><li>Data is scaled using zscore</ul></li>"
      ],
      "metadata": {
        "id": "gfnMUxNZ_8sg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "backend.clear_session()\n",
        "#Fixing the seed for random number generators so that we can ensure we receive the same output everytime\n",
        "np.random.seed(1)\n",
        "import random\n",
        "random.seed(1)\n",
        "tf.random.set_seed(1)"
      ],
      "metadata": {
        "id": "p6EBQ3k0AvQQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing the model\n",
        "model_04 = Sequential()\n",
        "# Adding the first hidden layer with 64 neurons, relu as activation function\n",
        "model_04.add(Dense(64, activation='relu', input_dim=11))\n",
        "# Adding the second hidden layer with 32 neurons, relu as activation function\n",
        "model_04.add(Dense(32, activation='relu'))\n",
        "# Adding the output layer with one neuron and Sigmoid as activation\n",
        "model_04.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compiling the model with 'cross entropy' as loss function, 'Adam' Optimizer and 'accuracy' metrics\n",
        "model_04.compile(loss='binary_crossentropy',\n",
        "              optimizer='SGD',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Fitting the model on train and validation with 20 epochs\n",
        "fitted_model_04 = model_04.fit(X_train_over, y_train_over, validation_split=0.2,epochs=20)\n",
        "\n",
        "#Plotting Train Loss vs Validation Loss\n",
        "plt.plot(fitted_model_04.history['loss'])\n",
        "plt.plot(fitted_model_04.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GU_HlPdwAzBW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is a high difference between error on train and validation set (reflects as overfit model)and noise are observed on the validation set. Let us calculate the remaining metrics to see if there is any improvement in the recall."
      ],
      "metadata": {
        "id": "yb1OwdHGA7Ng"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtain the y_predict from the X_test\n",
        "y_pred=model_04.predict(X_test)\n",
        "\n",
        "#set the threshold to 0.5\n",
        "y_pred = (y_pred > 0.5)\n",
        "\n",
        "# Display the classification report\n",
        "cr=metrics.classification_report(y_test,y_pred)\n",
        "print(cr)"
      ],
      "metadata": {
        "id": "Bhpe7Z8vBAsF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The recall has increased from 0.69 to 0.75 which is an advantage of using the oversampled data over the non-sampled training data. Let us finally explore the confusion matrix."
      ],
      "metadata": {
        "id": "dIzpRS34BPwt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculating the confusion matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm=confusion_matrix(y_test, y_pred)\n",
        "labels = ['True Positive','False Negative','False Positive','True Negative']\n",
        "categories = [ 'Not Exiting','Exiting']\n",
        "make_confusion_matrix(cm,\n",
        "                      group_names=labels,\n",
        "                      categories=categories,\n",
        "                      cmap='Blues')"
      ],
      "metadata": {
        "id": "SiGA8_M1Bi9_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The FN Count has increased significantly from 3% to 10%.<br><br>\n",
        "\n",
        "<b>Conclusion:</b><br><br>\n",
        "\n",
        "Using over sampled data enhances the performance of the model, yet it adds in lots of noise and increases the overfitting and the FN count.\n",
        "The SGD Optimizer still shows less noise and less overfitting in the model, hence it is perferred over the Adam optimizer.\n"
      ],
      "metadata": {
        "id": "AiciUHMOBxR8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b>Model_05:</b><br><br>\n",
        "In this model we will add 2 more hidden layer to have a total of 4 layers (160, 64, 32 and 16 Neurons respectively) and also add weight initialization using the he technique (since we are using the ReLU activation function).<br>\n",
        "<ul><li>Hidden layers = 4</ul></li>\n",
        "<ul><li>Activation function for hidden layers = Relu</ul></li>\n",
        "<ul><li>Weight initialization technique = he</ul></li>\n",
        "<ul><li>Output layer with 1 nodes</ul></li>\n",
        "<ul><li>Output Activation function = Sigmoid</ul></li>\n",
        "<ul><li>Output loss function = Cross Entropy (since this is a classification problem)</ul></li>\n",
        "<ul><li>Optimizer = SGD</ul></li>\n",
        "<ul><li>epochs = 20</ul></li>\n",
        "<ul><li>Data is scaled using zscore</ul></li>"
      ],
      "metadata": {
        "id": "THz34ug5Cxio"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "backend.clear_session()\n",
        "#Fixing the seed for random number generators so that we can ensure we receive the same output everytime\n",
        "np.random.seed(1)\n",
        "random.seed(1)\n",
        "tf.random.set_seed(1)"
      ],
      "metadata": {
        "id": "L2JZ6WmgDx3m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing the model\n",
        "model_05 = Sequential()\n",
        "# Adding the first hidden layer, relu as activation function and, he_uniform as weight initializer.\n",
        "model_05.add(Dense(160, activation='relu', input_dim=11, kernel_initializer='he_uniform'))\n",
        "# Adding the second hidden layer , relu as activation function and, he_uniform as weight initializer\n",
        "model_05.add(Dense(64, activation='relu', kernel_initializer='he_uniform'))\n",
        "# Adding the third hidden layer , relu as activation function and, he_uniform as weight initializer\n",
        "model_05.add(Dense(32, activation='relu', kernel_initializer='he_uniform'))\n",
        "# Adding the fourth hidden layer , relu as activation function and, he_uniform as weight initializer\n",
        "model_05.add(Dense(16, activation='relu', kernel_initializer='he_uniform'))\n",
        "\n",
        "# Adding the output layer with one neuron and Sigmoid as activation\n",
        "model_05.add(Dense(1, activation='sigmoid', kernel_initializer='he_uniform'))\n",
        "\n",
        "# Compiling the model with 'cross entropy' as loss function, 'Adam' Optimizer and 'accuracy' metrics\n",
        "model_05.compile(loss='binary_crossentropy',\n",
        "              optimizer='SGD',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Fitting the model on train and validation with 20 epochs\n",
        "fitted_model_05 = model_05.fit(X_train_over, y_train_over, validation_split=0.2,epochs=20)\n",
        "\n",
        "#Plotting Train Loss vs Validation Loss\n",
        "plt.plot(fitted_model_05.history['loss'])\n",
        "plt.plot(fitted_model_05.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KO1nGAq5D-9c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtain the y_predict from the X_test\n",
        "y_pred=model_05.predict(X_test)\n",
        "\n",
        "#set the threshold to 0.5\n",
        "y_pred = (y_pred > 0.5)\n",
        "\n",
        "# Display the classification report\n",
        "cr=metrics.classification_report(y_test,y_pred)\n",
        "print(cr)\n",
        "\n",
        "#Calculating the confusion matrix\n",
        "cm=confusion_matrix(y_test, y_pred)\n",
        "labels = ['True Positive','False Negative','False Positive','True Negative']\n",
        "categories = [ 'Not Exiting','Exiting']\n",
        "make_confusion_matrix(cm,\n",
        "                      group_names=labels,\n",
        "                      categories=categories,\n",
        "                      cmap='Blues')"
      ],
      "metadata": {
        "id": "EKkmlCYKEJbJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b>Observation:</b><br><br>\n",
        "\n",
        "The recall has decreased while the FN Ratio has stayed almost the same. The weight initialization does not seem to add any value in enhancing the model performance."
      ],
      "metadata": {
        "id": "tcyQAv7AEGih"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b>Model_06:</b><br>\n",
        "\n",
        "In this model we will increase the number of epochs to 50 in order to decrease the error values, use the oversampled data and remove the weight initializer.<br><br>\n",
        "<ul><li>Hidden layers = 4</ul></li>\n",
        "<ul><li>Activation function for hidden layers = Relu</ul></li>\n",
        "<ul><li>Output layer with 1 nodes</ul></li>\n",
        "<ul><li>Outout Activation function = Sigmoid</ul></li>\n",
        "<ul><li>Output loss function = Cross Entropy (since this is a classification problem)</ul></li>\n",
        "<ul><li>Optimizer = SGD</ul></li>\n",
        "<ul><li>epochs = 50</ul></li>\n",
        "<ul><li>Data is scaled using zscore</ul></li>"
      ],
      "metadata": {
        "id": "EQ0qN041Ekg4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "backend.clear_session()\n",
        "#Fixing the seed for random number generators so that we can ensure we receive the same output everytime\n",
        "np.random.seed(1)\n",
        "random.seed(1)\n",
        "tf.random.set_seed(1)"
      ],
      "metadata": {
        "id": "nNY0cKOXFI09"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing the model\n",
        "model_06 = Sequential()\n",
        "# Adding the first hidden layer, relu as activation function\n",
        "model_06.add(Dense(160, activation='relu', input_dim=11))\n",
        "# Adding the second hidden layer , relu as activation function\n",
        "model_06.add(Dense(64, activation='relu'))\n",
        "# Adding the third hidden layer , relu as activation function\n",
        "model_06.add(Dense(32, activation='relu'))\n",
        "# Adding the fourth hidden layer , relu as activation function\n",
        "model_06.add(Dense(16, activation='relu'))\n",
        "# Adding the output layer with one neuron and Sigmoid as activation\n",
        "model_06.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compiling the model with 'cross entropy' as loss function, 'Adam' Optimizer and 'accuracy' metrics\n",
        "model_06.compile(loss='binary_crossentropy',\n",
        "              optimizer='SGD',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Fitting the model on train and validation with 20 epochs\n",
        "fitted_model_06 = model_06.fit(X_train_over, y_train_over, validation_split=0.2,epochs=50)\n",
        "\n",
        "#Plotting Train Loss vs Validation Loss\n",
        "plt.plot(fitted_model_06.history['loss'])\n",
        "plt.plot(fitted_model_06.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SSW3hrlTFchR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The error value has decreased and seem to stabilize a bit for the validation set as the epochs increase, yet the noise observed on the validation set is higher with higher epochs. It seems that 50 epochs is the right value to stop at, yet we need to reduce the noise and overfit, hence we will apply some regulaziation techniques. Let us observe the performance metrics first."
      ],
      "metadata": {
        "id": "aicLQK7DGkPM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtain the y_predict from the X_test\n",
        "y_pred=model_06.predict(X_test)\n",
        "\n",
        "#set the threshold to 0.5\n",
        "y_pred = (y_pred > 0.5)\n",
        "\n",
        "# Display the classification report\n",
        "cr=metrics.classification_report(y_test,y_pred)\n",
        "print(cr)\n",
        "\n",
        "#Calculating the confusion matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm=confusion_matrix(y_test, y_pred)\n",
        "labels = ['True Positive','False Negative','False Positive','True Negative']\n",
        "categories = [ 'Not Exiting','Exiting']\n",
        "make_confusion_matrix(cm,\n",
        "                      group_names=labels,\n",
        "                      categories=categories,\n",
        "                      cmap='Blues')"
      ],
      "metadata": {
        "id": "188tsYzEHx2W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The recall has decreased and the FN ratio has decreased slightly. Yet the model is still showing overfit model and the noise on the validation set requires further reduction."
      ],
      "metadata": {
        "id": "Lkwu83DVILkb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b>Model_07:</b><br><br>\n",
        "In this model we will apply the Dropout Regularization technique while keeping all other hyperparameters and NN Structure fixed as in model_06<br><br>\n",
        "<ul><li>Hidden layers = 4</ul></li>\n",
        "<ul><li>Activation function for hidden layers = Relu</ul></li>\n",
        "<ul><li>Output layer with 1 nodes</ul></li>\n",
        "<ul><li>Output Activation function = Sigmoid</ul></li>\n",
        "<ul><li>Output loss function = Cross Entropy (since this is a classification problem)</ul></li>\n",
        "<ul><li>Optimizer = SGD</ul></li>\n",
        "<ul><li>epochs = 50</ul></li>\n",
        "<ul><li>Data is scaled using zscore</ul></li>\n",
        "<ul><li>Regularization technique: DropOut</ul></li>\n"
      ],
      "metadata": {
        "id": "v2bHY9arIN8F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "backend.clear_session()\n",
        "#Fixing the seed for random number generators so that we can ensure we receive the same output everytime\n",
        "np.random.seed(1)\n",
        "random.seed(1)\n",
        "tf.random.set_seed(1)"
      ],
      "metadata": {
        "id": "vIbdY8cQJVut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing the model\n",
        "model_07 = Sequential()\n",
        "# Adding the first hidden layer with 160 neurons, relu as activation function\n",
        "model_07.add(Dense(160, activation='relu', input_dim=11))\n",
        "# Adding dropout of 20%\n",
        "model_07.add(Dropout(0.2))\n",
        "# Adding the second hidden layer with 64 neurons, relu as activation function\n",
        "model_07.add(Dense(64, activation='relu', input_dim=11))\n",
        "# Adding dropout of 20%\n",
        "model_07.add(Dropout(0.2))\n",
        "# Adding the third hidden layer with 32 neurons, relu as activation function\n",
        "model_07.add(Dense(32, activation='relu'))\n",
        "# Adding dropout of 20%\n",
        "model_07.add(Dropout(0.2))\n",
        "# Adding the fourth hidden layer with 16 neurons, relu as activation function\n",
        "model_07.add(Dense(16, activation='relu'))\n",
        "# Adding dropout of 20%\n",
        "model_07.add(Dropout(0.2))\n",
        "# Adding the output layer with one neuron and Sigmoid as activation\n",
        "model_07.add(Dense(1, activation='sigmoid'))"
      ],
      "metadata": {
        "id": "o9WMBrnQJmP_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the Model summary to make sure all layers are well arranged\n",
        "model_07.summary()"
      ],
      "metadata": {
        "id": "G99fhsQWJrQG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compiling the model with 'cross entropy' as loss function, 'Adam' Optimizer and 'accuracy' metrics\n",
        "model_07.compile(loss='binary_crossentropy',\n",
        "              optimizer='SGD',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Fitting the model on train and validation with 50 epochs\n",
        "fitted_model_07 = model_07.fit(X_train_over, y_train_over, validation_split=0.2,epochs=50)\n",
        "\n",
        "#Plotting Train Loss vs Validation Loss\n",
        "plt.plot(fitted_model_07.history['loss'])\n",
        "plt.plot(fitted_model_07.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KDdXTedeJ8PX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The loss curve shows a significant reduction in noise on the validation set, yet the curve is still showing overfitting. Let us compute the performance metrics."
      ],
      "metadata": {
        "id": "Q9iUoeJ4KAH3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtain the y_predict from the X_test\n",
        "y_pred=model_07.predict(X_test)\n",
        "\n",
        "#set the threshold to 0.5\n",
        "y_pred = (y_pred > 0.5)\n",
        "\n",
        "# Display the classification report\n",
        "cr=metrics.classification_report(y_test,y_pred)\n",
        "print(cr)\n",
        "\n",
        "#Calculating the confusion matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm=confusion_matrix(y_test, y_pred)\n",
        "labels = ['True Positive','False Negative','False Positive','True Negative']\n",
        "categories = [ 'Not Exiting','Exiting']\n",
        "make_confusion_matrix(cm,\n",
        "                      group_names=labels,\n",
        "                      categories=categories,\n",
        "                      cmap='Blues')"
      ],
      "metadata": {
        "id": "5byETWr7KDRd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The recall has increased and there is a small increase in the FN ratio, yet the recall has enhanced to 0.75. Let us explore the AUC-ROC Curve for a better threshold."
      ],
      "metadata": {
        "id": "AtXnsZK8KOgB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve\n",
        "from matplotlib import pyplot\n",
        "\n",
        "# predict probabilities\n",
        "yhat1 = model_07.predict(X_test)\n",
        "# keep probabilities for the positive outcome only\n",
        "yhat1 = yhat1[:, 0]\n",
        "# calculate roc curves\n",
        "fpr, tpr, thresholds1 = roc_curve(y_test, yhat1)\n",
        "\n",
        "# plot the roc curve for the model\n",
        "pyplot.plot([0,1], [0,1], linestyle='--')\n",
        "pyplot.plot(fpr, tpr, marker='.')\n",
        "\n",
        "# axis labels\n",
        "pyplot.xlabel('False Positive Rate')\n",
        "pyplot.ylabel('True Positive Rate')\n",
        "\n",
        "# show the plot\n",
        "pyplot.show()"
      ],
      "metadata": {
        "id": "-aKKKFuUKPjt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we are searching for the threshold that will give us a good TPR value and still maintain. good FPR Value, we will aim for the threshold at TPR value close to 0.80"
      ],
      "metadata": {
        "id": "1SS95CsxKW31"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we will hunt for the threshold at TPR 0.8\n",
        "x = 0.80\n",
        "print(\"Value to which nearest element is to be found: \", x)\n",
        "\n",
        "# calculate the difference array\n",
        "difference_array = np.absolute(tpr-x)\n",
        "\n",
        "# find the index of minimum element from the array\n",
        "index = difference_array.argmin()\n",
        "print(\"Nearest element to the given values is : \", tpr[index])\n",
        "print(\"Index of nearest value is : \", index)\n",
        "print(\"Threshold at this index is:\", thresholds1[index])"
      ],
      "metadata": {
        "id": "ztacqIfDKap1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtain the y_predict from the X_test\n",
        "y_pred=model_07.predict(X_test)\n",
        "\n",
        "#set the threshold at tpr=0.8\n",
        "y_pred = (y_pred > thresholds1[index])\n",
        "\n",
        "# Display the classification report\n",
        "cr=metrics.classification_report(y_test,y_pred)\n",
        "print(cr)\n",
        "\n",
        "#Calculating the confusion matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm=confusion_matrix(y_test, y_pred)\n",
        "labels = ['True Positive','False Negative','False Positive','True Negative']\n",
        "categories = [ 'Not Exiting','Exiting']\n",
        "make_confusion_matrix(cm,\n",
        "                      group_names=labels,\n",
        "                      categories=categories,\n",
        "                      cmap='Blues')"
      ],
      "metadata": {
        "id": "o3lkJt_uKe6y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "By applying a very low threshold of value 0.26 that acheives a TPR of approx. 0.8 , we get the same recall value of 0.75 but we get a much much higher FN Ratio which is a drawback. Note that at threshold of 0.5 the recall value was the same (0.75) yet the FN ration was approx (10%), hence the threshold 0.5 is preferred over the threshold obtained from the AUC-ROC curve."
      ],
      "metadata": {
        "id": "aWeto-gtKmLw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b>Model_08:</b><br><br>\n",
        "In model_07 the Dropout regularization has shown a slight decrease in the overfitting, yet a noticible decrease in noise. The recall has not improved significantly and so, in this model we will apply the Batch normalization regularization techniqe. With that, we will use the non-scaled data X_train_ns, X_test_ns, y_train_ns and y_test_ns.<br><br>\n",
        "\n",
        "<ul><li>Hidden layers = 4</ul></li>\n",
        "<ul><li>Activation function for hidden layers = Relu</ul></li>\n",
        "<ul><li>Weight initialization technique = non</ul></li>\n",
        "<ul><li>Output layer with 1 nodes</ul></li>\n",
        "<ul><li>Outout Activation function = Sigmoid</ul></li>\n",
        "<ul><li>Output loss function = Cross Entropy (since this is a classification problem)</ul></li>\n",
        "<ul><li>Optimizer = SGD</ul></li>\n",
        "<ul><li>epochs = 50</ul></li>\n",
        "<ul><li>Data is not scaled</ul></li>\n",
        "<ul><li>Regularization technique: Batch</ul></li>"
      ],
      "metadata": {
        "id": "L3Wtk-4rK3Kt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "backend.clear_session()\n",
        "#Fixing the seed for random number generators so that we can ensure we receive the same output everytime\n",
        "np.random.seed(1)\n",
        "random.seed(1)\n",
        "tf.random.set_seed(1)"
      ],
      "metadata": {
        "id": "ClomlcoOLkwX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing the model\n",
        "model_08 = Sequential()\n",
        "# Adding the first hidden layer with relu as activation function\n",
        "model_08.add(Dense(160, activation='relu', input_dim=11))\n",
        "# Adding Batch Normalization\n",
        "model_08.add(BatchNormalization())\n",
        "# Adding the second hidden layer with relu as activation function\n",
        "model_08.add(Dense(64, activation='relu', input_dim=11))\n",
        "# Adding Batch Normalization\n",
        "model_08.add(BatchNormalization())\n",
        "# Adding the third hidden layer with relu as activation function\n",
        "model_08.add(Dense(32, activation='relu'))\n",
        "# Adding Batch Normalization\n",
        "model_08.add(BatchNormalization())\n",
        "# Adding the fourth hidden layer with relu as activation function\n",
        "model_08.add(Dense(16, activation='relu'))\n",
        "# Adding Batch Normalization\n",
        "model_08.add(BatchNormalization())\n",
        "# Adding the output layer with one neuron and Sigmoid as activation\n",
        "model_08.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compiling the model with 'cross entropy' as loss function, 'Adam' Optimizer and 'accuracy' metrics\n",
        "model_08.compile(loss='binary_crossentropy',\n",
        "              optimizer='SGD',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Fitting the model on train and validation with 20 epochs\n",
        "fitted_model_08 = model_08.fit(X_train_ns, y_train_ns, validation_split=0.2,epochs=50)\n",
        "\n",
        "#Plotting Train Loss vs Validation Loss\n",
        "plt.plot(fitted_model_08.history['loss'])\n",
        "plt.plot(fitted_model_08.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WIpp1nKsLoz9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The loss curve has dropped very rapidly and the error difference between the validation and training set has improved, yet not significantly. The error difference is stabilizing as well between the test and validation. There is also some noise observed. Let us compute the performance metrics."
      ],
      "metadata": {
        "id": "BGjS0qRBLywn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtain the y_predict from the X_test\n",
        "y_pred=model_08.predict(X_test_ns)\n",
        "\n",
        "#set the threshold to 0.5\n",
        "y_pred = (y_pred > 0.5)\n",
        "\n",
        "# Display the classification report\n",
        "cr=metrics.classification_report(y_test_ns,y_pred)\n",
        "print(cr)\n",
        "\n",
        "#Calculating the confusion matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm=confusion_matrix(y_test_ns, y_pred)\n",
        "labels = ['True Positive','False Negative','False Positive','True Negative']\n",
        "categories = [ 'Not Exiting','Exiting']\n",
        "make_confusion_matrix(cm,\n",
        "                      group_names=labels,\n",
        "                      categories=categories,\n",
        "                      cmap='Blues')"
      ],
      "metadata": {
        "id": "z12iNWx-L3XB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The recall has significantly decreased and the FN and TN completly vanishied, let us plot the ROC-AUC curve and see if changing the threshold can enhance this model."
      ],
      "metadata": {
        "id": "Yz3UdgUxMtLQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The ROC-AUC curve is reflecting a poor model, hence this explains the reduced recall and noise. Since, we have observed better performing models, we will ignore this one."
      ],
      "metadata": {
        "id": "8ike-aR7M7Im"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b>Model_09_with grid search:<b><br><br>\n",
        "In this model we will utilize the GridSearchCV to tune the learning rate and batch size. We will use the non-sampled training set to train the model and the below parameters:\n",
        "\n",
        "<ul><li>Hidden layers = 5</ul></li>\n",
        "<ul><li>Activation function for hidden layers = Relu</ul></li>\n",
        "<ul><li>Output layer with 1 nodes</ul></li>\n",
        "<ul><li>Output Activation function = Sigmoid</ul></li>\n",
        "<ul><li>Output loss function = Cross Entropy (since this is a classification problem)</ul></li>\n",
        "<ul><li>Optimizer = SGD</ul></li>\n",
        "<ul><li>epochs = 50</ul></li>\n",
        "<ul><li>Data is scaled using zscore</ul></li>\n",
        "<ul><li>Regularization technique: DropOut</ul></li>"
      ],
      "metadata": {
        "id": "TrPFuu9UM_i7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV"
      ],
      "metadata": {
        "id": "PayWGFKDOS8j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "backend.clear_session()\n",
        "#Fixing the seed for random number generators so that we can ensure we receive the same output everytime\n",
        "np.random.seed(1)\n",
        "random.seed(1)\n",
        "tf.random.set_seed(1)"
      ],
      "metadata": {
        "id": "JyqgRO0pOZ0s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model(lr,batch_size):\n",
        "    np.random.seed(1)\n",
        "    model = Sequential()\n",
        "    model.add(Dense(256,activation='relu',input_dim = 11))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(128,activation='relu'))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(65,activation='relu'))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(32,activation='relu'))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(16,activation='relu',kernel_initializer='he_uniform'))\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    #compile model\n",
        "    optimizer = tf.keras.optimizers.SGD(learning_rate=lr)\n",
        "    model.compile(optimizer = optimizer,loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "DLOTFE4RMvEt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
        "\n",
        "keras_estimator = KerasClassifier(build_fn=create_model, verbose=1)\n",
        "# define the grid search parameters\n",
        "param_grid = {\n",
        "    'batch_size':[64,32, 128, 256, 560, 1000],\n",
        "    \"lr\":[0.001,0.0015,0.0020,0.0025],}\n",
        "\n",
        "kfold_splits = 3\n",
        "grid = GridSearchCV(estimator=keras_estimator,\n",
        "                    verbose=1,\n",
        "                    cv=kfold_splits,\n",
        "                    param_grid=param_grid,n_jobs=-1)"
      ],
      "metadata": {
        "id": "A_D1xG5pOklX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "# store starting time\n",
        "begin = time.time()\n",
        "\n",
        "grid_result = grid.fit(X_train, y_train,validation_split=0.2,verbose=1)\n",
        "\n",
        "# Summarize results\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "\n",
        "time.sleep(1)\n",
        "# store end time\n",
        "end = time.time()\n",
        "\n",
        "# total time taken\n",
        "print(f\"Total runtime of the program is {end - begin}\")"
      ],
      "metadata": {
        "id": "j7y_F4wtOyB-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create the structure of the NN\n",
        "model_09=create_model(batch_size=grid_result.best_params_['batch_size'],lr=grid_result.best_params_['lr'])\n",
        "\n",
        "model_09.summary()"
      ],
      "metadata": {
        "id": "kH-cOCXUPCWY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare the optimizer for compiling\n",
        "optimizer = tf.keras.optimizers.SGD(grid_result.best_params_['lr'])\n",
        "model_09.compile(loss='binary_crossentropy',optimizer=optimizer,metrics=['accuracy'])\n",
        "\n",
        "# Fit the model to the train data\n",
        "fitted_model_09=model_09.fit(X_train, y_train, epochs=50, batch_size = 32, verbose=1,validation_split=0.2)"
      ],
      "metadata": {
        "id": "mbH1wObWPJOT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plotting Train Loss vs Validation Loss\n",
        "plt.plot(fitted_model_09.history['loss'])\n",
        "plt.plot(fitted_model_09.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BU5JS60rPNkd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The validation loss curve is much smoother and the overfit is less. Also the average loss value that the curves converge to is within a lower range than observed in the previous model."
      ],
      "metadata": {
        "id": "41jAOLDQPQd6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtain the y_predict from the X_test\n",
        "y_pred=model_09.predict(X_test)\n",
        "\n",
        "#set the threshold to 0.5\n",
        "y_pred = (y_pred > 0.5)\n",
        "\n",
        "# Display the classification report\n",
        "cr=metrics.classification_report(y_test,y_pred)\n",
        "print(cr)\n",
        "\n",
        "#Calculating the confusion matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm=confusion_matrix(y_test, y_pred)\n",
        "labels = ['True Positive','False Negative','False Positive','True Negative']\n",
        "categories = [ 'Not Exiting','Exiting']\n",
        "make_confusion_matrix(cm,\n",
        "                      group_names=labels,\n",
        "                      categories=categories,\n",
        "                      cmap='Blues')"
      ],
      "metadata": {
        "id": "sPy4l0xZPUvB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unfortunatly the recall has reduced to 0.54 and the FN and TN reduced significantly in the confusion matrix which makes this model unacceptable.\n",
        "\n",
        "Note: The GridSearchCV was trialed with the over sampled training set and the overfit was too high, hence the model was not included in this notebook.\n"
      ],
      "metadata": {
        "id": "GiCejOkAPhBo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b>Model_10</b><br><br>\n",
        "In this model we will use the Keras Tuner to guide us through the best combination of :\n",
        "<ul><li>No. of layers</ul></li>\n",
        "<ul><li>No. of neurons per layer</ul></li>\n",
        "<ul><li>Learning rate</ul></li>\n",
        "We will keep the other parameters as is just the regularization we remove:\n",
        "\n",
        "<ul><li>Hidden layers = 4</ul></li>\n",
        "<ul><li>Activation function for hidden layers = Relu</ul></li>\n",
        "<ul><li>Output layer with 1 nodes</ul></li>\n",
        "<ul><li>Outout Activation function = Sigmoid</ul></li>\n",
        "<ul><li>Output loss function = Cross Entropy (since this is a classification problem)</ul></li>\n",
        "<ul><li>Optimizer = SGD</ul></li>\n",
        "<ul><li>epochs = 50</ul></li>\n",
        "<ul><li>Data is scaled using zscore</ul></li>"
      ],
      "metadata": {
        "id": "jAubBdtYPjn7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## First we install Keras Tuner\n",
        "!pip install keras-tuner"
      ],
      "metadata": {
        "id": "9BtEI5KPRVKI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import the libraries\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from kerastuner.tuners import RandomSearch"
      ],
      "metadata": {
        "id": "WVg6fCbPR9e-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "backend.clear_session()\n",
        "np.random.seed(1)\n",
        "import random\n",
        "random.seed(1)\n",
        "tf.random.set_seed(1)"
      ],
      "metadata": {
        "id": "KMyICP62KbV_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(h):\n",
        "    model = Sequential()\n",
        "    for i in range(h.Int('num_layers', 2, 10)):\n",
        "        model.add(layers.Dense(units=h.Int('units_' + str(i),\n",
        "                                            min_value=32,\n",
        "                                            max_value=256,\n",
        "                                            step=32),\n",
        "                               activation='relu'))\n",
        "    model.add(layers.Dense(1, activation='sigmoid'))\n",
        "    model.compile(\n",
        "        optimizer=SGD(h.Choice('learning_rate', [1e-2, 1e-3, 1e-4])),\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['accuracy'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "Il4bX615SdP8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a tuner (here, RandomSearch). We use objective to specify the objective to select the best models,\n",
        "#  and we use max_trials to specify the number of different models to try\n",
        "\n",
        "tuner = RandomSearch(\n",
        "    build_model,\n",
        "    objective='val_accuracy',\n",
        "    max_trials=5,\n",
        "    executions_per_trial=3,\n",
        "     project_name='Job_')\n",
        "\n",
        "tuner.search_space_summary()\n",
        "\n",
        "# Print the best models with their hyperparameters\n",
        "tuner.results_summary()"
      ],
      "metadata": {
        "id": "k0BHf7i1Siqi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Searching the best model on X and y train\n",
        "tuner.search(X_train, y_train,\n",
        "             epochs=5,\n",
        "             validation_split = 0.2)\n",
        "INFO:tensorflow:Oracle triggered exit"
      ],
      "metadata": {
        "id": "HvOF31BKUAZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the best models with their hyperparameters\n",
        "tuner.results_summary()"
      ],
      "metadata": {
        "id": "2B0w8fupUDZs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It seems that the accuracy is almost the same for the number of layers 8 and 9. Also it is the same for learning rates 0.01, 0.001 and 0.0001. Hence, we choose the simpler model of 8 layers and the medium lerning rate of 0.001 to structure our model and observe different metrics."
      ],
      "metadata": {
        "id": "MFVolcY-Uz8N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "backend.clear_session()\n",
        "#Fixing the seed for random number generators so that we can ensure we receive the same output everytime\n",
        "np.random.seed(1)\n",
        "random.seed(1)\n",
        "tf.random.set_seed(1)"
      ],
      "metadata": {
        "id": "DPZ_gMxDU4tD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing the model\n",
        "model_10 = Sequential()\n",
        "# Adding the first hidden layer with relu as activation function\n",
        "model_10.add(Dense(160, activation='relu', input_dim=11))\n",
        "# Adding the second hidden layer with relu as activation function\n",
        "model_10.add(Dense(224, activation='relu'))\n",
        "# Adding the third hidden layer with relu as activation function\n",
        "model_10.add(Dense(160, activation='relu'))\n",
        "# Adding the fourth hidden layer with relu as activation function\n",
        "model_10.add(Dense(32, activation='relu'))\n",
        "# Adding the fifth hidden layer with relu as activation function\n",
        "model_10.add(Dense(192, activation='relu'))\n",
        "# Adding the sixth hidden layer with relu as activation function\n",
        "model_10.add(Dense(96, activation='relu'))\n",
        "# Adding the seventh hidden layer with relu as activation function\n",
        "model_10.add(Dense(192, activation='relu'))\n",
        "# Adding the eighth hidden layer with relu as activation function\n",
        "model_10.add(Dense(160, activation='relu'))\n",
        "# Adding the output layer with one neuron and Sigmoid as activation\n",
        "model_10.add(Dense(1, activation='sigmoid'))"
      ],
      "metadata": {
        "id": "42RhgQsOU7qg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_10.summary()"
      ],
      "metadata": {
        "id": "Quadso3EVA9H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = SGD(0.001)\n",
        "model_10.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "fFCo6VMEVGqj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fitting the model on train and validation with 50 epochs\n",
        "fitted_model_10 = model_10.fit(X_train, y_train, validation_split=0.2,epochs=50)"
      ],
      "metadata": {
        "id": "8p37yLvZVJeS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plotting Train Loss vs Validation Loss\n",
        "plt.plot(fitted_model_10.history['loss'])\n",
        "plt.plot(fitted_model_10.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "w011H3x_VO_z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The validation and training loss curves are showing the smoothest trend in this tuning method. The noise is minimal and the overfitting is much less compared to the Random grid search. Let us compute the performance metrics and observe if the model performance has enhanced as well."
      ],
      "metadata": {
        "id": "_EPZLNGtVe5f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtain the y_predict from the X_test\n",
        "y_pred=model_10.predict(X_test)\n",
        "\n",
        "#set the threshold to 0.202 from trialing different thresholds\n",
        "y_pred = (y_pred > 0.202)\n",
        "\n",
        "# Display the classification report\n",
        "cr=metrics.classification_report(y_test,y_pred)\n",
        "print(cr)\n",
        "\n",
        "#Calculating the confusion matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm=confusion_matrix(y_test, y_pred)\n",
        "labels = ['True Positive','False Negative','False Positive','True Negative']\n",
        "categories = [ 'Not Exiting','Exiting']\n",
        "make_confusion_matrix(cm,\n",
        "                      group_names=labels,\n",
        "                      categories=categories,\n",
        "                      cmap='Blues')"
      ],
      "metadata": {
        "id": "ywAX9xAdVmwk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The recall is reduced to 0.69 and the FN Ratio has increased significantly to 25%. This shows that the tuned NN structure has not enhanced the model performance and that the over sampled data introduces noise and overfit yet higher recall."
      ],
      "metadata": {
        "id": "NfCVme7HVvNw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b>Model_11</b><br><br>\n",
        "same as model_10 yet with dropout"
      ],
      "metadata": {
        "id": "WMEOtcobV-nT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "backend.clear_session()\n",
        "#Fixing the seed for random number generators so that we can ensure we receive the same output everytime\n",
        "np.random.seed(1)\n",
        "random.seed(1)\n",
        "tf.random.set_seed(1)"
      ],
      "metadata": {
        "id": "c9B03F_DWNdv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model_w_dropout(h):\n",
        "    model = Sequential()\n",
        "    for i in range(h.Int('num_layers', 2, 10)):\n",
        "        model.add(layers.Dense(units=h.Int('units_' + str(i),\n",
        "                                            min_value=32,\n",
        "                                            max_value=256,\n",
        "                                            step=32),\n",
        "                               activation='relu'))\n",
        "        model.add(Dropout(0.2))\n",
        "    model.add(layers.Dense(1, activation='sigmoid'))\n",
        "    model.compile(\n",
        "        optimizer=SGD(h.Choice('learning_rate', [1e-2, 1e-3, 1e-4])),\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['accuracy'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "acfpwpI8WPnd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a tuner (here, RandomSearch). We use objective to specify the objective to select the best models,\n",
        "#  and we use max_trials to specify the number of different models to try\n",
        "\n",
        "tuner_2 = RandomSearch(\n",
        "    build_model_w_dropout,\n",
        "    objective='val_accuracy',\n",
        "    max_trials=5,\n",
        "    executions_per_trial=3,\n",
        "     project_name='Job_')"
      ],
      "metadata": {
        "id": "YD9JIvq_WWhe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tuner_2.search_space_summary()"
      ],
      "metadata": {
        "id": "YwSE3SmlWb17"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Searching the best model on X and y train\n",
        "tuner_2.search(X_train, y_train,\n",
        "             epochs=5,\n",
        "             validation_split = 0.2)"
      ],
      "metadata": {
        "id": "7KB3LDIqWe7W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tuner_2.search_space_summary()"
      ],
      "metadata": {
        "id": "oFXI-MZwWkXJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing the model and adding drop out\n",
        "model_11 = Sequential()\n",
        "# Adding the first hidden layer with 224 neurons, relu as activation function\n",
        "model_11.add(Dense(224, activation='relu', input_dim=11))\n",
        "# Adding dropout of 20%\n",
        "model_11.add(Dropout(0.2))\n",
        "# Adding the second hidden layer with 160 neurons, relu as activation function\n",
        "model_11.add(Dense(160, activation='relu'))\n",
        "# Adding dropout of 20%\n",
        "model_11.add(Dropout(0.2))\n",
        "# Adding the third hidden layer with 32 neurons, relu as activation function\n",
        "model_11.add(Dense(32, activation='relu'))\n",
        "# Adding dropout of 20%\n",
        "model_11.add(Dropout(0.2))\n",
        "# Adding the fourth hidden layer with 192 neurons, relu as activation function\n",
        "model_11.add(Dense(192, activation='relu'))\n",
        "# Adding dropout of 20%\n",
        "model_11.add(Dropout(0.2))\n",
        "# Adding the fifth hidden layer with 96 neurons, relu as activation function\n",
        "model_11.add(Dense(96, activation='relu'))\n",
        "# Adding dropout of 20%\n",
        "model_11.add(Dropout(0.2))\n",
        "# Adding the sixth hidden layer with 192 neurons, relu as activation function\n",
        "model_11.add(Dense(192, activation='relu'))\n",
        "# Adding dropout of 20%\n",
        "model_11.add(Dropout(0.2))\n",
        "# Adding the seventh hidden layer with 160 neurons, relu as activation function\n",
        "model_11.add(Dense(160, activation='relu'))\n",
        "# Adding dropout of 20%\n",
        "model_11.add(Dropout(0.2))\n",
        "# Adding the eighth hidden layer with 160 neurons, relu as activation function\n",
        "model_11.add(Dense(160, activation='relu'))\n",
        "# Adding dropout of 20%\n",
        "model_11.add(Dropout(0.2))\n",
        "# Adding the output layer with one neuron and Sigmoid as activation\n",
        "model_11.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "optimizer = SGD(0.001)\n",
        "model_11.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "QyAxljmzWuhL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fitting the model on train and validation with 50 epochs\n",
        "fitted_model_11 = model_11.fit(X_train, y_train, validation_split=0.2,epochs=50)\n",
        "\n",
        "#Plotting Train Loss vs Validation Loss\n",
        "plt.plot(fitted_model_11.history['loss'])\n",
        "plt.plot(fitted_model_11.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vWO5DCr9W0LA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtain the y_predict from the X_test\n",
        "y_pred=model_11.predict(X_test)\n",
        "\n",
        "#set the threshold to 0.202\n",
        "y_pred = (y_pred > 0.202)\n",
        "\n",
        "# Display the classification report\n",
        "cr=metrics.classification_report(y_test,y_pred)\n",
        "print(cr)\n",
        "\n",
        "#Calculating the confusion matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm=confusion_matrix(y_test, y_pred)\n",
        "labels = ['True Positive','False Negative','False Positive','True Negative']\n",
        "categories = [ 'Not Exiting','Exiting']\n",
        "make_confusion_matrix(cm,\n",
        "                      group_names=labels,\n",
        "                      categories=categories,\n",
        "                      cmap='Blues')"
      ],
      "metadata": {
        "id": "Wnrfp3lOW-A8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model performance has actually worsened in both Recall and FN Ratio. Hence, we will apply oversampling on the train test (SMOTE) and observe if it improves the model performance."
      ],
      "metadata": {
        "id": "85MAYXVmXB8g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b>Model_12:</b><br><br>\n",
        "As the Keras Tuner with Drop out showed less overfitting model, we will train it on oversampled train data set using SMOTE algorithm. All other hyperparameters are fixed:"
      ],
      "metadata": {
        "id": "Q6l2ukKIXGkV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "backend.clear_session()\n",
        "#Fixing the seed for random number generators so that we can ensure we receive the same output everytime\n",
        "np.random.seed(1)\n",
        "random.seed(1)\n",
        "tf.random.set_seed(1)"
      ],
      "metadata": {
        "id": "O1qCtqJtXL53"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model_smote(h):\n",
        "    model = keras.Sequential()\n",
        "    for i in range(h.Int('num_layers', 2, 10)):\n",
        "        model.add(layers.Dense(units=h.Int('units_' + str(i),\n",
        "                                            min_value=32,\n",
        "                                            max_value=256,\n",
        "                                            step=32),\n",
        "                               activation='relu'))\n",
        "    model.add(layers.Dense(1, activation='sigmoid'))\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(\n",
        "            h.Choice('learning_rate', [1e-2, 1e-3, 1e-4])),\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['accuracy'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "-oVIVd5qXa6_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tuner_3 = RandomSearch(\n",
        "    build_model_smote,\n",
        "    objective='val_accuracy',\n",
        "    max_trials=5,\n",
        "    executions_per_trial=3,\n",
        "    project_name='Job_Switch')"
      ],
      "metadata": {
        "id": "r8qd2IwyXfvW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tuner_2.search_space_summary()"
      ],
      "metadata": {
        "id": "6wwAeydcXjTY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tuner_3.search(X_train_over, y_train_over,\n",
        "             epochs=5,\n",
        "             validation_split = 0.2)"
      ],
      "metadata": {
        "id": "804UmWQKXoVA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tuner_3.results_summary()"
      ],
      "metadata": {
        "id": "R9NPDztLXq1h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "backend.clear_session()\n",
        "#Fixing the seed for random number generators so that we can ensure we receive the same output everytime\n",
        "np.random.seed(1)\n",
        "random.seed(1)\n",
        "tf.random.set_seed(1)"
      ],
      "metadata": {
        "id": "DCghu9tZXzIL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# building the NN Based on the hyperparameters recommended by the Keras Tuner\n",
        "model_12 = Sequential()\n",
        "# Adding the first hidden layer with 160 neurons, relu as activation function\n",
        "model_12.add(Dense(160, activation='relu', input_dim=11))\n",
        "# Adding dropout of 20%\n",
        "# model_11.add(Dropout(0.2))\n",
        "# Adding the second hidden layer with 224 neurons, relu as activation function\n",
        "model_12.add(Dense(224, activation='relu'))\n",
        "# Adding dropout of 20%\n",
        "# model_11.add(Dropout(0.2))\n",
        "# Adding the third hidden layer with 160 neurons, relu as activation function\n",
        "model_12.add(Dense(160, activation='relu'))\n",
        "# Adding dropout of 20%\n",
        "# model_11.add(Dropout(0.2))\n",
        "# Adding the fourth hidden layer with 32 neurons, relu as activation function\n",
        "model_12.add(Dense(32, activation='relu'))\n",
        "# Adding dropout of 20%\n",
        "# model_11.add(Dropout(0.2))\n",
        "# Adding the fifth hidden layer with 192 neurons, relu as activation function\n",
        "model_12.add(Dense(192, activation='relu'))\n",
        "# Adding dropout of 20%\n",
        "# model_11.add(Dropout(0.2))\n",
        "# Adding the sixth hidden layer with 96 neurons, relu as activation function\n",
        "model_12.add(Dense(96, activation='relu'))\n",
        "# Adding dropout of 20%\n",
        "# model_11.add(Dropout(0.2))\n",
        "# Adding the seventh hidden layer with 192 neurons, relu as activation function\n",
        "model_12.add(Dense(192, activation='relu'))\n",
        "# Adding dropout of 20%\n",
        "model_12.add(Dropout(0.2))\n",
        "# Adding the eighth hidden layer with 160 neurons, relu as activation function\n",
        "model_12.add(Dense(160, activation='relu'))\n",
        "# Adding dropout of 20%\n",
        "# model_11.add(Dropout(0.2))\n",
        "# Adding the output layer with one neuron and Sigmoid as activation\n",
        "model_12.add(Dense(1, activation='sigmoid'))"
      ],
      "metadata": {
        "id": "cDU-MhUiX2Hu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compiling the model with 'cross entropy' as loss function, 'Adam' Optimizer and 'accuracy' metrics\n",
        "model_12.compile(loss='binary_crossentropy',\n",
        "              optimizer='SGD',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "7bRlPDLsX-ws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fitted_model_12 = model_12.fit(X_train_over, y_train_over, validation_split=0.2,epochs=50)"
      ],
      "metadata": {
        "id": "jBDIXNDbYCA8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plotting Train Loss vs Validation Loss\n",
        "plt.plot(fitted_model_12.history['loss'])\n",
        "plt.plot(fitted_model_12.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Dpe2QJM1YInE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The noise in the validation set is still quiet high (expected though since we are using the oversampled data) yet the overfitting is less than observed in earlier models. Also, the valudation loss curve is fluctuating around a lower loss average value when compared to earlier models."
      ],
      "metadata": {
        "id": "wnpNDiLCYLcN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtain the y_predict from the X_test\n",
        "y_pred=model_12.predict(X_test)\n",
        "\n",
        "#set the threshold to 0.5\n",
        "y_pred = (y_pred > 0.5)\n",
        "\n",
        "# Display the classification report\n",
        "cr=metrics.classification_report(y_test,y_pred)\n",
        "print(cr)\n",
        "\n",
        "#Calculating the confusion matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm=confusion_matrix(y_test, y_pred)\n",
        "labels = ['True Positive','False Negative','False Positive','True Negative']\n",
        "categories = [ 'Not Exiting','Exiting']\n",
        "make_confusion_matrix(cm,\n",
        "                      group_names=labels,\n",
        "                      categories=categories,\n",
        "                      cmap='Blues')"
      ],
      "metadata": {
        "id": "aOZ4pFVVYPRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As expected, the model performance is higher than the recall from the model trained on the non-sampled data. Also, the FN has decreased to 10% which is also an improvement from the latest model."
      ],
      "metadata": {
        "id": "QtrCuHA7YVW8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b>Conclusion and key takeaways</b><br><br>\n",
        "<b>Key observations</b><br>\n",
        "<ul><li>The majority of model gave recall between 0.69 to 0.75 with few models showed a drop to recall value 0.5</ul></li>\n",
        "<ul><li>The FN Ratio was very challenging to reduce as at the majority of models it was inversly proportional with the recall (recall value enhances and FN value worsens)</ul></li>\n",
        "<ul><li>Utilizing the oversampled data entroduced noise in the validation dataset loss curve, yet it supported the model to a slightly better performance</ul></li>\n",
        "<ul><li>It was observed from the EDA that the data set target classes were imbalanced.</ul></li>\n",
        "<ul><li>It was also observed from the EDA that the majority of features are in fact very poor predictors</ul></li>"
      ],
      "metadata": {
        "id": "KjbdwHQTYZdc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b>Key Conclusion</b><br><br>\n",
        "<ul><li>The best performing model is: Model_07 which gives a recall of 0.75, slight overfit, acceptable noise and accpetable FN ratio od 7%.</ul></li>\n",
        "<ul><li>As a result of data set features poor predictibility power and the high imbalance in the target variable, the NN modelling algorithm could acheive a maximum best of Recall value 0.75 with slight overfitting.</ul></li>\n",
        "<ul><li>Utilizing this model will support the bank in identifying the customers who are willing to exit the bank and acocrdingly early action plans can be put in place to attain these customers (please see the EDA Insights section where the common features of exiting customers are stated)</ul></li>"
      ],
      "metadata": {
        "id": "N3IulT7wYvBL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The curves are still smooth and showing less overfitting, let us explore the performance parameters"
      ],
      "metadata": {
        "id": "ko7QLs6DW5j_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "os7Vhv_HVkSh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "alIqmSlSJ77P"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "da_q5-CdGlW9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "1khbjUseEZvl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "TbIFzZimELi5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "nk-4OsFYELc1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b>\n",
        "\n",
        "Hidden layers = 4\n",
        "Activation function for hidden layers = Relu\n",
        "Weight initialization technique = he\n",
        "Output layer with 1 nodes\n",
        "Output Activation function = Sigmoid\n",
        "Output loss function = Cross Entropy (since this is a classification problem)\n",
        "Optimizer = SGD\n",
        "epochs = 20\n",
        "Data is scaled using zscore"
      ],
      "metadata": {
        "id": "ZWtuHcwXCZSy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "FAkIF92l-bel"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EWHL2NEr-Uga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bfG8A-sy-PGx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Yt_Ep4VP9xBF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "3k8bN4S58pBb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "A1-6Z5p66JPK"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Rv2_Smmd34oU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "j9FUShFQrZTT"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OhYXkGcVxHyj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "P-j01-SUqvzb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CnKxBjH3qTt4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "65VPWDDCpukZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FHHrSIl4c6Yn"
      },
      "source": [
        "## Importing necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kpyBzcQsYYSh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7ubXtC8HUOA"
      },
      "source": [
        "## Loading the dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xGZLJmZUdkPq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Overview"
      ],
      "metadata": {
        "id": "eRxrJ2MHd_Sf"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4sPbCEoLuQBk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W036jsgwRdVN"
      },
      "source": [
        "## Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Univariate Analysis"
      ],
      "metadata": {
        "id": "nSFkV8KJiZSv"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g1Lxry70ibDw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bivariate Analysis"
      ],
      "metadata": {
        "id": "OlHTHF4glMxS"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y_pKBXS9lLel"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUXPaUwZHUO8"
      },
      "source": [
        "## Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1nEEjgwleiMv"
      },
      "source": [
        "### Dummy Variable Creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LJ19WoAYc6Yx"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train-validation-test Split"
      ],
      "metadata": {
        "id": "wgpx0xlSTlzN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VTb3JwlaHUO-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qlSyq5fNHUPp"
      },
      "source": [
        "### Data Normalization"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ltsmUrMiuegY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Building"
      ],
      "metadata": {
        "id": "ZLQMVCywT87j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Evaluation Criterion"
      ],
      "metadata": {
        "id": "SzDpHlsFT_QA"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FayG94iciXVS"
      },
      "source": [
        "Write down the logic for choosing the metric that would be the best metric for this business scenario.\n",
        "\n",
        "-\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3drHnx12DSf2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "735HwSYiDSf1"
      },
      "source": [
        "### Neural Network with SGD Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ScqNP3QjDSf3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Performance Improvement"
      ],
      "metadata": {
        "id": "ygzPHkE_Anaw"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EcEiT7Vyc6Y0"
      },
      "source": [
        "### Neural Network with Adam Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_5anPDAru0Vq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Neural Network with Adam Optimizer and Dropout"
      ],
      "metadata": {
        "id": "I-86J6fRu0vu"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2WkE_mqIu0SP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Neural Network with Balanced Data (by applying SMOTE) and SGD Optimizer"
      ],
      "metadata": {
        "id": "m1Hav_XNu6ro"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vAHO1_vYu0DN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Neural Network with Balanced Data (by applying SMOTE) and Adam Optimizer"
      ],
      "metadata": {
        "id": "sFhOUq6au-xp"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8egmXgW0u-Q0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Neural Network with Balanced Data (by applying SMOTE), Adam Optimizer, and Dropout"
      ],
      "metadata": {
        "id": "rTSK-_5YvBIR"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3Jqa4h2yuuE7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srfZVuMKc6Y_"
      },
      "source": [
        "## Model Performance Comparison and Final Model Selection"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_vhQsZT5MR_9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XE1iHOqqOEmV"
      },
      "source": [
        "## Actionable Insights and Business Recommendations"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*\n",
        "\n"
      ],
      "metadata": {
        "id": "ouNNiEhUdhZL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font size=6 color='blue'>Power Ahead</font>\n",
        "___"
      ],
      "metadata": {
        "id": "6R23W-K3CmM9"
      }
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "nSFkV8KJiZSv",
        "OlHTHF4glMxS",
        "1nEEjgwleiMv",
        "wgpx0xlSTlzN",
        "EcEiT7Vyc6Y0",
        "I-86J6fRu0vu",
        "m1Hav_XNu6ro",
        "sFhOUq6au-xp",
        "srfZVuMKc6Y_",
        "XE1iHOqqOEmV"
      ],
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}